{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Geriatric Health Data Generation - Google Colab\n",
                "\n",
                "This notebook generates 50,000 synthetic training samples using:\n",
                "- **Local Qwen 2.5 3B** (runs on Colab's free GPU/CPU)\n",
                "- **Gemini API** (for 10% of calls, when quota available)\n",
                "\n",
                "**Runtime**: ~8-12 hours on Colab free tier\n",
                "\n",
                "## Instructions\n",
                "1. **Upload your seed files**: `intents.json`, `claude.json`, `gemini.json` to Colab\n",
                "2. **Set your Gemini API key** in the cell below\n",
                "3. **Run all cells**\n",
                "4. **Download** `synthetic_geriatric_data.jsonl` when complete"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q google-generativeai openai pandas psutil"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install and start Ollama in Colab\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!nohup ollama serve > /tmp/ollama.log 2>&1 &\n",
                "!sleep 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pull the lightweight Qwen model\n",
                "!ollama pull qwen2.5:3b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload your seed files\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Upload your seed JSON files (intents.json, claude.json, gemini.json):\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "import os\n",
                "print(\"\\nUploaded files:\")\n",
                "for filename in uploaded.keys():\n",
                "    print(f\"  - {filename} ({len(uploaded[filename])} bytes)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your Gemini API key\n",
                "import os\n",
                "\n",
                "# REPLACE WITH YOUR ACTUAL API KEY\n",
                "os.environ['GEMINI_API_KEY'] = 'AIzaSyAF0jRW5m446-N6Gp8UO_vWo8HYQWaAyIk'\n",
                "\n",
                "print(\"âœ“ API key configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data generation script (Colab version)\n",
                "import json\n",
                "import time\n",
                "import gc\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "from google import genai\n",
                "from google.genai import types\n",
                "from openai import OpenAI\n",
                "\n",
                "# Configuration\n",
                "GEMINI_MODEL = 'gemini-2.0-flash-exp'  # Faster model with higher quota\n",
                "QWEN_MODEL = 'qwen2.5:3b'\n",
                "OUTPUT_FILE = 'synthetic_geriatric_data.jsonl'\n",
                "TARGET_SIZE = 50000\n",
                "GEMINI_RATIO = 0.10  # 10% Gemini, 90% Qwen\n",
                "GEMINI_DAILY_QUOTA = 100  # Higher on Colab usually\n",
                "\n",
                "# Initialize clients\n",
                "gemini_client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
                "qwen_client = OpenAI(\n",
                "    api_key='EMPTY',\n",
                "    base_url='http://localhost:11434/v1',\n",
                "    timeout=30.0,\n",
                "    max_retries=1\n",
                ")\n",
                "\n",
                "print(\"âœ“ Clients initialized\")\n",
                "\n",
                "# Schema\n",
                "ALPAC_SCHEMA = types.Schema(\n",
                "    type=types.Type.ARRAY,\n",
                "    items=types.Schema(\n",
                "        type=types.Type.OBJECT,\n",
                "        properties={\n",
                "            \"instruction\": types.Schema(type=types.Type.STRING),\n",
                "            \"input\": types.Schema(type=types.Type.STRING),\n",
                "            \"output\": types.Schema(type=types.Type.STRING)\n",
                "        },\n",
                "        required=[\"instruction\", \"input\", \"output\"]\n",
                "    )\n",
                ")\n",
                "\n",
                "MASTER_SYSTEM_PROMPT = \"\"\"You are an AI Geriatric Health Assistant. Provide safe, simple, non-diagnostic wellness and medical advice for elderly patients (70+ years). For severe symptoms, advise emergency services. Never diagnose or suggest stopping medication. Generate 10 unique instruction/input/output JSON triples per prompt.\"\"\"\n",
                "\n",
                "# Load seed data\n",
                "def load_seed_data():\n",
                "    all_seeds = []\n",
                "    \n",
                "    for filename in ['intents.json', 'claude.json', 'gemini.json']:\n",
                "        if not Path(filename).exists():\n",
                "            continue\n",
                "        \n",
                "        with open(filename, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        \n",
                "        # Handle different formats\n",
                "        if isinstance(data, list):\n",
                "            for item in data[:100]:  # Sample 100 from each\n",
                "                if 'instruction' in item:\n",
                "                    all_seeds.append(item)\n",
                "        elif isinstance(data, dict) and 'intents' in data:\n",
                "            for intent in data['intents'][:50]:\n",
                "                patterns = intent.get('patterns', [])\n",
                "                responses = intent.get('responses', [])\n",
                "                if patterns and responses:\n",
                "                    all_seeds.append({\n",
                "                        'instruction': patterns[0],\n",
                "                        'input': 'Patient seeking conversational advice.',\n",
                "                        'output': responses[0]\n",
                "                    })\n",
                "    \n",
                "    print(f\"Loaded {len(all_seeds)} seed samples\")\n",
                "    return all_seeds\n",
                "\n",
                "seeds = load_seed_data()\n",
                "\n",
                "# Topics\n",
                "topics = [\n",
                "    \"Hydration and dietary advice for seniors\",\n",
                "    \"Managing chronic pain (arthritis, backaches)\",\n",
                "    \"Fall prevention and home safety\",\n",
                "    \"Medication management tips\",\n",
                "    \"Signs of common elderly illnesses\",\n",
                "    \"Sleep and fatigue management\",\n",
                "    \"Memory and cognitive engagement\",\n",
                "    \"Diabetes management for seniors\",\n",
                "    \"Heart health for 70+\",\n",
                "    \"Senior-safe exercise\",\n",
                "    \"Treating minor cuts and burns\",\n",
                "    \"Managing headaches safely\",\n",
                "    \"Dealing with nosebleeds\",\n",
                "    \"Sore throat relief\",\n",
                "    \"Digestive issues\",\n",
                "    \"Eye strain and vision comfort\",\n",
                "    \"Loneliness and emotional well-being\",\n",
                "    \"Stress and anxiety management\",\n",
                "    \"Sleep problems and insomnia\",\n",
                "    \"Coping with grief and loss\"\n",
                "]\n",
                "\n",
                "print(\"\\nðŸš€ Starting generation...\")\n",
                "print(f\"Target: {TARGET_SIZE} samples\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main generation loop\n",
                "generated_count = 0\n",
                "batch_count = 0\n",
                "gemini_calls = 0\n",
                "\n",
                "import random\n",
                "random.seed(42)\n",
                "\n",
                "while generated_count < TARGET_SIZE:\n",
                "    topic = topics[generated_count % len(topics)]\n",
                "    seed_sample = json.dumps(seeds[:5], indent=2)\n",
                "    \n",
                "    prompt = f\"\"\"Generate 10 NEW unique instruction/input/output JSON triples about: {topic}\n",
                "    \n",
                "Examples:\n",
                "{seed_sample}\"\"\"\n",
                "    \n",
                "    # Decide which model to use\n",
                "    use_gemini = (gemini_calls < GEMINI_DAILY_QUOTA and random.random() < GEMINI_RATIO)\n",
                "    model_name = GEMINI_MODEL if use_gemini else QWEN_MODEL\n",
                "    \n",
                "    print(f\"[Batch {batch_count+1}] {model_name} | {topic[:50]}...\")\n",
                "    \n",
                "    try:\n",
                "        if use_gemini:\n",
                "            response = gemini_client.models.generate_content(\n",
                "                model=model_name,\n",
                "                contents=prompt,\n",
                "                config=types.GenerateContentConfig(\n",
                "                    system_instruction=MASTER_SYSTEM_PROMPT,\n",
                "                    response_mime_type=\"application/json\",\n",
                "                    response_schema=ALPAC_SCHEMA,\n",
                "                    temperature=0.8\n",
                "                )\n",
                "            )\n",
                "            triples = json.loads(response.text)\n",
                "            gemini_calls += 1\n",
                "        else:\n",
                "            response = qwen_client.chat.completions.create(\n",
                "                model=model_name,\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": MASTER_SYSTEM_PROMPT},\n",
                "                    {\"role\": \"user\", \"content\": prompt}\n",
                "                ],\n",
                "                response_format={\"type\": \"json_object\"},\n",
                "                temperature=0.8,\n",
                "                max_tokens=1500\n",
                "            )\n",
                "            data = json.loads(response.choices[0].message.content)\n",
                "            triples = data if isinstance(data, list) else list(data.values())[0]\n",
                "        \n",
                "        # Save valid triples\n",
                "        valid = [t for t in triples if all(k in t for k in [\"instruction\", \"input\", \"output\"])]\n",
                "        \n",
                "        with open(OUTPUT_FILE, 'a') as f:\n",
                "            for item in valid:\n",
                "                f.write(json.dumps(item) + \"\\n\")\n",
                "        \n",
                "        generated_count += len(valid)\n",
                "        batch_count += 1\n",
                "        \n",
                "        print(f\"  âœ“ {len(valid)} triples | Progress: {generated_count}/{TARGET_SIZE} ({100*generated_count/TARGET_SIZE:.1f}%)\")\n",
                "        \n",
                "        # Light cooldown\n",
                "        time.sleep(2 if use_gemini else 10)\n",
                "        \n",
                "        # Progress checkpoint every 100 batches\n",
                "        if batch_count % 100 == 0:\n",
                "            print(f\"\\nðŸ“Š Checkpoint: {generated_count} samples, {gemini_calls} Gemini calls\\n\")\n",
                "            gc.collect()\n",
                "        \n",
                "    except Exception as e:\n",
                "        if \"429\" in str(e):\n",
                "            gemini_calls = GEMINI_DAILY_QUOTA\n",
                "            print(f\"  âš ï¸ Gemini quota hit\")\n",
                "        else:\n",
                "            print(f\"  âŒ Error: {str(e)[:100]}\")\n",
                "        time.sleep(10)\n",
                "\n",
                "print(f\"\\nâœ… COMPLETE! Generated {generated_count} samples\")\n",
                "print(f\"File: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the results\n",
                "from google.colab import files\n",
                "\n",
                "print(f\"Downloading {OUTPUT_FILE}...\")\n",
                "files.download(OUTPUT_FILE)\n",
                "print(\"âœ“ Download complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}