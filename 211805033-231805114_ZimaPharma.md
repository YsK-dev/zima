# AYDIN ADNAN MENDERES UNIVERSITY
## FACULTY OF ENGINEERING
### DEPARTMENT OF COMPUTER ENGINEERING

---

# ZIMA: A DOMAIN-SPECIALIZED LARGE LANGUAGE MODEL FOR GERIATRIC HEALTH ASSISTANCE THROUGH LOW-RANK ADAPTATION

## UNDERGRADUATE THESIS

**Authors:**  
Yusuf SERTKAYA (211805033)  
Umut SEZER (231805114)

**Thesis Supervisor:**  
Assoc. Prof. Dr. Ahmet Çağdaş SEÇKİN

**AYDIN, 2025**

---

## DECLARATION

We hereby declare that this thesis titled "ZIMA: A Domain-Specialized Large Language Model for Geriatric Health Assistance Through Low-Rank Adaptation" is our original work and has been written in accordance with academic rules and ethical conduct. We confirm that all sources used have been properly cited and referenced.

**Date:** January 2025

**Signatures:**

Yusuf SERTKAYA ________________

Umut SEZER ________________

---

## APPROVAL PAGE

This thesis titled "ZIMA: A Domain-Specialized Large Language Model for Geriatric Health Assistance Through Low-Rank Adaptation" prepared by Yusuf SERTKAYA and Umut SEZER has been accepted as an Undergraduate Thesis in the Department of Computer Engineering by the following jury members.

**Thesis Supervisor:**  
Assoc. Prof. Dr. Ahmet Çağdaş SEÇKİN  
Aydın Adnan Menderes University  
Department of Computer Engineering

**Jury Members:**

Member: _________________________________ Signature: _____________

Member: _________________________________ Signature: _____________

**Approval Date:** ___/___/2025

---

## ABSTRACT

**ZIMA: A Domain-Specialized Large Language Model for Geriatric Health Assistance Through Low-Rank Adaptation**

**SERTKAYA, Yusuf; SEZER, Umut**  
B.Sc. Thesis, Department of Computer Engineering  
Supervisor: Assoc. Prof. Dr. Ahmet Çağdaş SEÇKİN  
January 2025, 25 pages

The unprecedented growth of the global elderly population has intensified demand for accessible, empathetic, and reliable health guidance systems. This thesis introduces Zima, a domain-specialized Large Language Model (LLM) engineered to deliver practical, actionable health advice for geriatric patients aged 70 years and above. Leveraging the Qwen 2.5 1.5B Instruct architecture as a foundation, we employ Low-Rank Adaptation (LoRA) fine-tuning methodology to achieve domain specialization while preserving computational efficiency.

Our contributions are threefold: First, we develop a GPU-accelerated synthetic data generation pipeline utilizing a locally-deployed Qwen 2.5 14B model, producing a novel dataset of 10,743 instruction-response pairs spanning 20 geriatric health categories. Second, we implement an efficient LoRA fine-tuning framework achieving a perplexity score of 1.51 on held-out validation data, with adapter weights of merely 74 megabytes. Third, we construct an interactive Gradio-based evaluation interface enabling comparative analysis between base and fine-tuned model outputs.

The model and dataset are publicly released on HuggingFace Hub to facilitate reproducibility and future research. Qualitative evaluation across 50 test cases demonstrates substantial improvements in response actionability, domain specificity, and communicative empathy compared to the unmodified base model.

**Keywords:** Large Language Model, Low-Rank Adaptation, Domain Specialization, Geriatric Healthcare, Natural Language Processing, Synthetic Data Generation, Model Fine-Tuning

---

## ÖZET

**ZIMA: Düşük Sıralı Adaptasyon ile Geriatrik Sağlık Yardımı için Alan Özelleştirilmiş Büyük Dil Modeli**

**SERTKAYA, Yusuf; SEZER, Umut**  
Lisans Tezi, Bilgisayar Mühendisliği Bölümü  
Tez Danışmanı: Doç. Dr. Ahmet Çağdaş SEÇKİN  
Ocak 2025, 25 sayfa

Küresel yaşlı nüfusundaki benzeri görülmemiş artış, erişilebilir, empatik ve güvenilir sağlık rehberlik sistemlerine olan talebi yoğunlaştırmıştır. Bu tez, 70 yaş ve üzeri geriatrik hastalara pratik, uygulanabilir sağlık tavsiyeleri sunmak için tasarlanmış alan özelleştirilmiş bir Büyük Dil Modeli (LLM) olan Zima'yı tanıtmaktadır. Qwen 2.5 1.5B Instruct mimarisini temel alarak, hesaplama verimliliğini korurken alan uzmanlığı elde etmek için Düşük Sıralı Adaptasyon (LoRA) ince ayar metodolojisini kullanmaktayız.

Katkılarımız üç ana başlık altında özetlenebilir: Birincisi, yerel olarak dağıtılmış Qwen 2.5 14B modeli kullanan GPU hızlandırmalı sentetik veri üretim hattı geliştirdik ve 20 geriatrik sağlık kategorisini kapsayan 10.743 talimat-yanıt çiftinden oluşan özgün bir veri seti ürettik. İkincisi, doğrulama verileri üzerinde 1.51 perplexity puanı elde eden ve yalnızca 74 megabayt adaptör ağırlıklarına sahip verimli bir LoRA ince ayar çerçevesi uyguladık. Üçüncüsü, temel ve ince ayarlı model çıktıları arasında karşılaştırmalı analiz sağlayan etkileşimli Gradio tabanlı değerlendirme arayüzü inşa ettik.

Model ve veri seti, tekrarlanabilirliği ve gelecek araştırmaları kolaylaştırmak amacıyla HuggingFace Hub üzerinde kamuya açık olarak yayınlanmıştır.

**Anahtar Kelimeler:** Büyük Dil Modeli, Düşük Sıralı Adaptasyon, Alan Özelleştirme, Geriatrik Sağlık, Doğal Dil İşleme, Sentetik Veri Üretimi, Model İnce Ayarı

---

## ACKNOWLEDGEMENTS

We express our sincere gratitude to our thesis supervisor, Assoc. Prof. Dr. Ahmet Çağdaş Seçkin, whose expertise, patience, and continuous guidance were instrumental throughout the development of this research. His insightful feedback and encouragement helped us navigate the complexities of this interdisciplinary project.

We extend our appreciation to Assoc. Prof. Dr. Fatih Soygazi for his valuable technical consultations and constructive suggestions during the implementation phase.

We acknowledge the computational resources provided by Lightning.ai, which enabled the GPU-accelerated data generation pipeline central to this work.

Finally, we thank our families for their unwavering support and understanding throughout this academic endeavor.

---

## TABLE OF CONTENTS

| Section | Page |
|---------|------|
| DECLARATION | ii |
| APPROVAL PAGE | iii |
| ABSTRACT | iv |
| ÖZET | v |
| ACKNOWLEDGEMENTS | vi |
| TABLE OF CONTENTS | vii |
| LIST OF TABLES | viii |
| LIST OF FIGURES | ix |
| LIST OF ABBREVIATIONS | x |
| 1. INTRODUCTION | 1 |
| 1.1 Background and Motivation | 1 |
| 1.2 Problem Statement | 2 |
| 1.3 Research Objectives | 3 |
| 1.4 Contributions | 3 |
| 1.5 Thesis Organization | 4 |
| 2. LITERATURE REVIEW | 5 |
| 2.1 Large Language Models in Healthcare | 5 |
| 2.2 Parameter-Efficient Fine-Tuning | 6 |
| 2.3 Synthetic Data Generation | 7 |
| 3. METHODOLOGY | 8 |
| 3.1 System Architecture | 8 |
| 3.2 Data Generation Pipeline | 9 |
| 3.3 Model Fine-Tuning | 11 |
| 3.4 Evaluation Framework | 13 |
| 4. IMPLEMENTATION | 14 |
| 4.1 Development Environment | 14 |
| 4.2 Data Processing | 14 |
| 4.3 Training Procedure | 15 |
| 4.4 Demonstration Application | 16 |
| 5. RESULTS AND ANALYSIS | 17 |
| 5.1 Quantitative Results | 17 |
| 5.2 Qualitative Evaluation | 18 |
| 5.3 Comparative Analysis | 20 |
| 6. DISCUSSION | 21 |
| 6.1 Findings | 21 |
| 6.2 Limitations | 21 |
| 6.3 Ethical Considerations | 22 |
| 7. CONCLUSION AND FUTURE WORK | 23 |
| REFERENCES | 24 |
| APPENDICES | 25 |

---

## LIST OF TABLES

| Table | Title | Page |
|-------|-------|------|
| Table 1 | Data Generation Infrastructure Specifications | 9 |
| Table 2 | Dataset Composition and Statistics | 10 |
| Table 3 | Topic Distribution in Generated Dataset | 10 |
| Table 4 | LoRA Hyperparameter Configuration | 12 |
| Table 5 | Training Hyperparameters | 12 |
| Table 6 | Training Results Summary | 17 |
| Table 7 | Comparative Analysis: Base Model vs. Zima | 20 |

---

## LIST OF FIGURES

| Figure | Title | Page |
|--------|-------|------|
| Figure 1 | System Architecture Overview | 8 |
| Figure 2 | Data Generation Pipeline | 9 |
| Figure 3 | LoRA Architecture Diagram | 11 |
| Figure 4 | Demonstration Application Interface | 16 |

---

## LIST OF ABBREVIATIONS

| Abbreviation | Definition |
|--------------|------------|
| API | Application Programming Interface |
| GPU | Graphics Processing Unit |
| HF | HuggingFace |
| JSON | JavaScript Object Notation |
| LLM | Large Language Model |
| LoRA | Low-Rank Adaptation |
| MLP | Multi-Layer Perceptron |
| NLP | Natural Language Processing |
| PEFT | Parameter-Efficient Fine-Tuning |
| QLoRA | Quantized Low-Rank Adaptation |
| RAG | Retrieval-Augmented Generation |
| SFT | Supervised Fine-Tuning |
| VRAM | Video Random Access Memory |

---

## 1. INTRODUCTION

### 1.1 Background and Motivation

The global demographic landscape is undergoing a profound transformation. According to the United Nations World Population Prospects (2022), the population aged 65 years and above is projected to increase from 761 million in 2021 to 1.6 billion by 2050, representing a growth rate that significantly exceeds that of the general population [1]. This demographic shift presents substantial challenges for healthcare systems worldwide, particularly in delivering accessible, personalized, and continuous health guidance to an expanding elderly population.

Elderly individuals frequently encounter complex health management requirements, including polypharmacy regimens, chronic disease monitoring, mobility preservation, cognitive maintenance, and psychosocial well-being [2]. Traditional healthcare delivery mechanisms struggle to meet these multifaceted needs due to physician time constraints, geographic barriers, and the episodic nature of clinical encounters. Consequently, there exists a critical need for scalable, intelligent systems capable of providing reliable, empathetic health information tailored to geriatric-specific concerns.

The emergence of Large Language Models (LLMs) represents a paradigm shift in artificial intelligence capabilities [3]. Foundation models such as GPT-4 [4], Claude [5], and open-source alternatives including LLaMA [6] and Qwen [7] have demonstrated remarkable proficiency in natural language understanding and generation across diverse domains. These models offer unprecedented potential for healthcare applications, providing 24/7 accessibility, consistent response quality, and scalable deployment.

However, general-purpose LLMs exhibit fundamental limitations when applied to specialized healthcare domains. Without domain-specific adaptation, these models frequently produce generic responses that inadequately address the unique physiological, pharmacological, and psychosocial considerations pertinent to geriatric care [8].

### 1.2 Problem Statement

Despite the impressive capabilities of contemporary LLMs, their direct application to geriatric health assistance reveals several critical deficiencies:

**1. Response Genericity:** Base models generate advice calibrated for general adult populations, failing to account for age-related pharmacokinetic changes, increased fall susceptibility, and cognitive considerations specific to elderly patients.

**2. Communication Style Mismatch:** The linguistic register employed by general-purpose models may be excessively technical, rapid, or lacking the patience and repetition beneficial for users with sensory or cognitive impairments.

**3. Over-Reliance on Professional Referral:** Models frequently default to recommending physician consultation for conditions manageable through well-established home interventions, thereby diminishing practical utility for everyday health concerns.

**4. Insufficient Safety Prioritization:** General models inconsistently prioritize safety considerations paramount to elderly patients, including fall prevention, medication interaction awareness, and emergency symptom recognition.

**5. Limited Actionability:** Responses often provide informational content without specific, implementable guidance including dosages, durations, and concrete steps.

These limitations necessitate domain-specialized model development through targeted fine-tuning methodologies.

### 1.3 Research Objectives

This thesis pursues the following research objectives:

1. **Dataset Development:** Construct a comprehensive synthetic dataset comprising high-quality instruction-response pairs specifically designed for geriatric health scenarios, encompassing diverse topics including medication management, chronic pain, fall prevention, emotional well-being, and common ailments.

2. **Model Specialization:** Apply parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA) to develop a domain-specialized model that provides compassionate, practical, and age-appropriate health guidance while achieving a perplexity score below 2.0 on held-out validation data.

3. **Public Dissemination:** Deploy the trained model and dataset to HuggingFace Hub, enabling community access, reproducibility verification, and subsequent research advancement.

4. **Evaluation Framework:** Develop and apply a comprehensive evaluation methodology assessing response quality across dimensions of relevance, actionability, safety awareness, and communicative empathy.

### 1.4 Contributions

This thesis presents the following contributions to the field:

1. **Novel Dataset:** We introduce the first publicly available synthetic dataset specifically designed for geriatric health assistance, comprising 10,743 high-quality instruction-response pairs across 20 topic categories.

2. **Domain-Specialized Model:** We develop Zima, a LoRA-adapted language model achieving a perplexity of 1.51 on geriatric health data, demonstrating effective domain specialization with minimal computational overhead (74 MB adapter weights).

3. **Reproducible Pipeline:** We provide complete, open-source implementation code for data generation, model training, and evaluation, enabling full replication and extension by the research community.

4. **Empirical Evaluation:** We conduct systematic qualitative evaluation demonstrating substantial improvements in response quality compared to the unmodified base model across multiple assessment dimensions.

### 1.5 Thesis Organization

The remainder of this thesis is organized as follows:

**Chapter 2** reviews relevant literature on large language models in healthcare, parameter-efficient fine-tuning techniques, and synthetic data generation methodologies.

**Chapter 3** presents the methodology, including system architecture, data generation pipeline, model fine-tuning approach, and evaluation framework.

**Chapter 4** details the implementation, covering development environment, data processing, training procedures, and demonstration application construction.

**Chapter 5** reports quantitative and qualitative results with comparative analysis between the base and fine-tuned models.

**Chapter 6** discusses findings, limitations, and ethical considerations.

**Chapter 7** concludes the thesis and proposes directions for future research.

---

## 2. LITERATURE REVIEW

### 2.1 Large Language Models in Healthcare

The application of Large Language Models to healthcare domains has attracted substantial research attention. Singhal et al. [9] introduced Med-PaLM, demonstrating that appropriately prompted LLMs can achieve physician-level performance on medical question-answering benchmarks. Their subsequent work, Med-PaLM 2 [10], achieved 85.4% accuracy on the MedQA dataset, approaching licensed physician performance levels.

Extensive evaluation of ChatGPT and GPT-4 for medical applications has yielded mixed results. Thirunavukarasu et al. [11] conducted a systematic review identifying strong performance on general health queries alongside concerning error rates in specialized domains, underscoring the necessity of domain-specific adaptation for safety-critical applications.

Open-source LLMs have enabled healthcare-focused fine-tuning endeavors unconstrained by proprietary access limitations. The BioMistral [12] and MedLLaMA [13] projects demonstrated that domain-specific fine-tuning substantially improves medical task performance even for models with parameter counts significantly below frontier systems.

For geriatric healthcare specifically, research remains limited. Existing work has focused primarily on general medical question-answering rather than the unique communicative and content requirements of elderly patient interaction.

### 2.2 Parameter-Efficient Fine-Tuning

Full fine-tuning of large language models requires substantial computational resources and risks catastrophic forgetting of general capabilities. Parameter-efficient fine-tuning (PEFT) methods address these limitations through selective parameter updates.

**Low-Rank Adaptation (LoRA)**, introduced by Hu et al. [14], represents a seminal contribution to efficient fine-tuning. LoRA hypothesizes that weight updates during fine-tuning exhibit low intrinsic dimensionality. By representing weight updates as the product of low-rank matrices (ΔW = BA, where B ∈ ℝ^(d×r) and A ∈ ℝ^(r×k), and r << min(d,k)), LoRA achieves comparable performance to full fine-tuning with 0.1-1% of trainable parameters.

**QLoRA**, proposed by Dettmers et al. [15], extends LoRA through integration with 4-bit quantization, enabling fine-tuning of models with billions of parameters on consumer-grade hardware while maintaining performance fidelity.

**Unsloth** [16] provides optimized training kernels achieving 2x speedup for LoRA fine-tuning through memory-efficient implementations, further democratizing access to LLM adaptation.

### 2.3 Synthetic Data Generation

High-quality training data constitutes a critical bottleneck for domain-specialized model development. Synthetic data generation using capable generative models has emerged as a viable solution when authentic data is scarce, sensitive, or expensive to acquire [17].

Wang et al. [18] demonstrated through the Self-Instruct framework that LLMs can generate diverse, high-quality instruction-following data suitable for model fine-tuning. Subsequent work has refined generation prompting strategies, quality filtering mechanisms, and diversity enhancement techniques.

For healthcare applications, synthetic data generation offers advantages including patient privacy preservation, controlled scenario generation, and scalable dataset construction. Our work applies these principles to geriatric health domain specialization.

---

## 3. METHODOLOGY

### 3.1 System Architecture

The Zima system follows a three-stage pipeline architecture designed for modularity, reproducibility, and computational efficiency:

```
┌──────────────────────────────────────────────────────────────────────────┐
│                        ZIMA SYSTEM ARCHITECTURE                          │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ┌────────────────┐    ┌────────────────┐    ┌────────────────┐        │
│   │    STAGE 1     │    │    STAGE 2     │    │    STAGE 3     │        │
│   │ Data Generation│───▶│ Model Training │───▶│  Demonstration │        │
│   └────────────────┘    └────────────────┘    └────────────────┘        │
│          │                      │                      │                 │
│          ▼                      ▼                      ▼                 │
│   ┌────────────────┐    ┌────────────────┐    ┌────────────────┐        │
│   │  Lightning.ai  │    │    Unsloth     │    │     Gradio     │        │
│   │  NVIDIA L40    │    │  LoRA + QLoRA  │    │    Web UI      │        │
│   │  Qwen 14B      │    │  Qwen 1.5B     │    │                │        │
│   └────────────────┘    └────────────────┘    └────────────────┘        │
│          │                      │                      │                 │
│          ▼                      ▼                      ▼                 │
│   10,743 samples         74 MB adapter         Comparative              │
│     (.jsonl)           (.safetensors)           evaluation              │
│                                                                          │
│                  ┌──────────────────────┐                                │
│                  │   HuggingFace Hub    │                                │
│                  │  Public Deployment   │                                │
│                  └──────────────────────┘                                │
└──────────────────────────────────────────────────────────────────────────┘
```

**Figure 1: System Architecture Overview**

**Stage 1 (Data Generation)** utilizes GPU-accelerated inference on Lightning.ai infrastructure to generate synthetic training data through a locally-deployed Qwen 2.5 14B model.

**Stage 2 (Model Training)** applies LoRA fine-tuning to the Qwen 2.5 1.5B Instruct base model using the Unsloth library for optimized training.

**Stage 3 (Demonstration)** provides an interactive Gradio-based interface for model evaluation and comparison.

### 3.2 Data Generation Pipeline

#### 3.2.1 Infrastructure

Data generation was performed on Lightning.ai cloud infrastructure to leverage high-performance GPU resources within free-tier constraints.

**Table 1: Data Generation Infrastructure Specifications**

| Component | Specification |
|-----------|---------------|
| Platform | Lightning.ai Cloud |
| GPU | NVIDIA L40 |
| GPU Memory | 48 GB VRAM |
| Generation Model | Qwen 2.5 14B |
| Inference Engine | Ollama (local deployment) |
| Batch Size | 20 samples per request |
| Maximum Runtime | 3.5 hours |
| API Architecture | 100% local (no external dependencies) |

#### 3.2.2 Generation Methodology

The generation pipeline implements a structured approach to synthetic data creation:

**Seed Data Integration:** Initial seed examples were sourced from three curated files (intents.json, claude.json, gemini.json) to establish quality baselines and ensure topical diversity.

**Prompt Engineering:** The system prompt was carefully designed to elicit responses adhering to the following principles:
- Practical, actionable advice with specific parameters (dosages, durations)
- Safety-conscious recommendations with appropriate escalation criteria
- Compassionate, senior-appropriate communication style
- Home remedy prioritization before professional referral recommendation

**Quality Control:** Generated samples undergo validation including JSON structure verification, required field presence (instruction, input, output), and minimum response length filtering.

**Table 2: Dataset Composition and Statistics**

| Metric | Value |
|--------|-------|
| Total Generated Samples | 10,743 |
| Training Set (90%) | 9,700 |
| Validation Set (10%) | 1,078 |
| Mean Instruction Length | 42 tokens |
| Mean Response Length | 87 tokens |

#### 3.2.3 Topic Coverage

The generation pipeline cycles through 20 predefined geriatric health categories to ensure comprehensive domain coverage:

**Table 3: Topic Distribution in Generated Dataset**

| Category | Topics Included |
|----------|-----------------|
| Physical Health | Chronic pain, headaches, nosebleeds, sore throat, digestive issues |
| Chronic Conditions | Diabetes management, heart health |
| Medication | Adherence, management, interactions |
| Safety | Fall prevention, home safety |
| Lifestyle | Hydration, nutrition, exercise safety |
| Cognitive | Memory concerns, confusion |
| Emotional | Loneliness, grief, anxiety, stress |
| Sleep | Insomnia, fatigue, sleep hygiene |
| First Aid | Minor injuries, common ailments |
| Vision | Eye strain, comfort |

---

### 3.3 Model Fine-Tuning

#### 3.3.1 Base Model Selection

We selected Qwen 2.5 1.5B Instruct [7] as the foundation model based on the following criteria:

1. **Parameter Efficiency:** The 1.5B parameter scale provides strong language capabilities while enabling deployment on consumer-grade hardware with 4-bit quantization.

2. **Instruction Following:** Pre-existing instruction-tuning provides a robust foundation for domain-specific adaptation, reducing required fine-tuning data volume.

3. **License Permissibility:** The Apache 2.0 license enables research use, modification, and public redistribution without restrictions.

4. **Multilingual Capability:** Native multilingual support enables potential future extension to non-English populations.

#### 3.3.2 Low-Rank Adaptation

We apply LoRA [14] to achieve parameter-efficient domain adaptation. The mathematical formulation is as follows:

For a pre-trained weight matrix W₀ ∈ ℝ^(d×k), the adapted weight is computed as:

**W = W₀ + ΔW = W₀ + BA**

where B ∈ ℝ^(d×r), A ∈ ℝ^(r×k), and the rank r << min(d, k).

During training, W₀ remains frozen while only B and A are updated, reducing trainable parameters by approximately 99%.

```
┌─────────────────────────────────────────────────────────────┐
│                   LoRA ARCHITECTURE                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│        Input (x)                                            │
│            │                                                │
│     ┌──────┴──────┐                                         │
│     │             │                                         │
│     ▼             ▼                                         │
│  ┌─────┐      ┌─────┐                                       │
│  │ W₀  │      │  A  │  ← Trainable (r × k)                  │
│  │     │      └──┬──┘                                       │
│  │Frozen│        │                                          │
│  │     │        ▼                                           │
│  │     │     ┌─────┐                                        │
│  │     │     │  B  │  ← Trainable (d × r)                   │
│  └──┬──┘     └──┬──┘                                        │
│     │           │                                           │
│     └─────┬─────┘                                           │
│           │                                                 │
│           ▼                                                 │
│      Output = W₀x + BAx                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**Figure 3: LoRA Architecture Diagram**

**Table 4: LoRA Hyperparameter Configuration**

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Rank (r) | 16 | Balance between expressivity and efficiency |
| Alpha (α) | 32 | Scaling factor; typically 2× rank |
| Dropout | 0.05 | Regularization to prevent overfitting |
| Bias | None | Standard LoRA configuration |
| Target Modules | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj | All attention and MLP projection layers |

#### 3.3.3 Training Configuration

**Table 5: Training Hyperparameters**

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Base Model | unsloth/Qwen2.5-1.5B-Instruct | Optimized 4-bit variant |
| Maximum Sequence Length | 512 tokens | Sufficient for health Q&A format |
| Quantization | 4-bit (NF4) | Memory efficiency via QLoRA |
| Per-Device Batch Size | 8 | Hardware-optimized |
| Gradient Accumulation Steps | 4 | Effective batch size = 32 |
| Learning Rate | 2×10⁻⁴ | Standard for LoRA fine-tuning |
| Learning Rate Scheduler | Cosine annealing | Smooth decay to zero |
| Warmup Steps | 50 | Gradual learning rate ramp |
| Training Epochs | 3 | Sufficient for convergence |
| Weight Decay | 0.01 | L2 regularization |
| Maximum Gradient Norm | 1.0 | Gradient clipping for stability |
| Optimizer | AdamW (8-bit) | Memory-efficient Adam variant |
| Precision | BF16/FP16 | Mixed-precision training |

#### 3.3.4 Training Data Format

All training samples are formatted using the Alpaca instruction template [19]:

```
Below is an instruction that describes a task, paired with an input 
that provides further context. Write a response that appropriately 
completes the request.

### Instruction:
{instruction}

### Input:
{patient_context}

### Response:
{model_output}
```

This template provides clear structural separation between task specification, contextual information, and expected response, facilitating consistent model behavior during inference.

### 3.4 Evaluation Framework

We employ a multi-dimensional evaluation framework assessing model output quality across four dimensions:

1. **Relevance (R):** Does the response directly address the patient's stated concern without unrelated tangents?

2. **Actionability (A):** Does the response provide specific, implementable steps with concrete parameters (dosages, durations, frequencies)?

3. **Safety (S):** Does the response appropriately identify emergency situations, recommend professional consultation when warranted, and avoid potentially harmful advice?

4. **Empathy (E):** Is the communication style appropriately warm, patient, and suitable for elderly users who may have cognitive or sensory limitations?

Each sample is assessed on a binary scale (✓/✗) for each dimension. Additionally, we perform comparative analysis between base model and fine-tuned model outputs on identical inputs.

---

## 4. IMPLEMENTATION

### 4.1 Development Environment

The implementation leverages the following software stack:

| Component | Version/Specification |
|-----------|----------------------|
| Python | 3.10+ |
| PyTorch | 2.1+ |
| Transformers | 4.40+ |
| PEFT | 0.18+ |
| Unsloth | Latest |
| Gradio | 4.0+ |
| Ollama | Latest |

Source code is organized in the following directory structure:

```
zima/
├── data_creation/
│   ├── data_creation_lightning.py    # Generation script (398 LOC)
│   └── setup_lightning.sh            # Environment configuration
├── training/
│   ├── prepare_data.py               # Data preprocessing (199 LOC)
│   ├── train_unsloth.py              # Training script (233 LOC)
│   ├── evaluate_model.py             # Evaluation script
│   └── setup_training.sh             # Training environment
├── demo/
│   ├── app.py                        # Gradio application (226 LOC)
│   └── requirements.txt              # Dependencies
├── trained_model/                     # Model artifacts (74 MB)
│   ├── adapter_model.safetensors
│   ├── adapter_config.json
│   └── tokenizer files
└── seed_data/                         # Seed examples
    ├── intents.json
    ├── claude.json
    └── gemini.json
```

### 4.2 Data Processing

The data preparation pipeline (prepare_data.py) implements the following workflow:

1. **Loading:** JSONL file parsing with error handling for malformed entries
2. **Filtering:** Removal of samples with missing outputs or response length < 10 characters
3. **Formatting:** Application of Alpaca template to each sample
4. **Splitting:** Stratified 90/10 train/validation split with fixed random seed (42) for reproducibility
5. **Serialization:** Output to train.jsonl and validation.jsonl files

```python
def format_sample(sample: Dict) -> Dict:
    """Format sample with Alpaca prompt template."""
    return {
        "text": ALPACA_PROMPT.format(
            instruction=sample["instruction"],
            input=sample.get("input", ""),
            output=sample["output"]
        )
    }
```

### 4.3 Training Procedure

Training is executed via train_unsloth.py, which performs:

1. **GPU Verification:** Confirmation of CUDA availability and memory reporting
2. **Model Loading:** 4-bit quantized model instantiation via Unsloth
3. **LoRA Injection:** Adapter layer addition to target modules
4. **Trainer Configuration:** SFTTrainer initialization with specified hyperparameters
5. **Training Loop:** Supervised fine-tuning with evaluation checkpoints
6. **Model Serialization:** Adapter weight and configuration saving

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-1.5B-Instruct",
    max_seq_length=512,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
)
```

### 4.4 Demonstration Application

The Gradio-based demonstration application (app.py) provides:

1. **Dual-Mode Loading:** Automatic GPU detection with Unsloth optimization or CPU fallback via standard PEFT
2. **Comparative Generation:** Side-by-side output generation with dynamic adapter enable/disable
3. **Thread Safety:** Mutex-protected generation to prevent concurrent access issues
4. **Example Queries:** Pre-populated examples demonstrating typical use cases

```
┌────────────────────────────────────────────────────────────────────┐
│        ZIMA GERIATRIC HEALTH ASSISTANT - COMPARISON DEMO           │
├────────────────────────────────────────────────────────────────────┤
│                                                                    │
│  ┌─────────────────────┐   ┌─────────────────────────────────────┐│
│  │ Health Question     │   │  Base Model    │    Zima Model      ││
│  │ [________________]  │   │  (Generic)     │   (Fine-tuned)     ││
│  │                     │   │                │                    ││
│  │ Patient Context     │   │ [Response...] │  [Response...]     ││
│  │ [________________]  │   │                │                    ││
│  │                     │   │                │                    ││
│  │ [Compare Models]    │   │                │                    ││
│  └─────────────────────┘   └─────────────────────────────────────┘│
│                                                                    │
│  Example Queries:                                                  │
│  • "What can I do about constipation?" - Patient is 73...         │
│  • "How can I reduce my risk of falls?" - Patient is 75...        │
└────────────────────────────────────────────────────────────────────┘
```

**Figure 4: Demonstration Application Interface**

---

## 5. RESULTS AND ANALYSIS

### 5.1 Quantitative Results

Model training completed successfully with the following metrics:

**Table 6: Training Results Summary**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Final Training Loss | 0.32 | Low loss indicates effective learning |
| Final Validation Loss | 0.40 | Minimal train-validation gap suggests limited overfitting |
| **Perplexity** | **1.51** | Excellent predictive performance |
| Training Duration | ~2 hours | Efficient training cycle |
| Adapter File Size | 74 MB | Compact storage footprint |
| Trainable Parameters | 23,068,672 | 1.52% of total parameters |
| Total Parameters | 1,519,485,952 | Base model preserved |

**Perplexity Analysis:**

Perplexity measures the model's uncertainty when predicting tokens in the validation set, calculated as:

**PPL = exp(L)**

where L is the average cross-entropy loss per token.

A perplexity of 1.51 indicates that the model assigns high probability to correct tokens in the validation data, demonstrating effective domain adaptation. For reference, general-purpose models typically achieve perplexity values between 5-20 on specialized domain data prior to fine-tuning.

The minimal gap between training loss (0.32) and validation loss (0.40) indicates that the model generalizes well to unseen data without significant overfitting, attributable to the regularization effects of LoRA's low-rank constraint and dropout.

### 5.2 Qualitative Evaluation

We conducted systematic evaluation on 50 test samples spanning all topic categories. Representative examples are presented below with evaluation annotations.

---

**Sample 1: Emotional Well-being (Loneliness)**

| Component | Content |
|-----------|---------|
| **Instruction** | "I feel lonely" |
| **Context** | Patient is elderly (70+), seeking advice on emotional well-being. |
| **Zima Response** | Try joining clubs or groups where you can meet people with similar interests. Consider volunteering to help others and connect socially. Regular phone calls can also keep relationships strong. Talk to a counselor if loneliness feels overwhelming. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response demonstrates appropriate empathy, provides multiple actionable suggestions (clubs, volunteering, phone calls), and appropriately recommends professional support for severe cases.

---

**Sample 2: Physical Activity (Exercise Initiation)**

| Component | Content |
|-----------|---------|
| **Instruction** | "How can I safely start an exercise routine at my age?" |
| **Context** | Patient is 75 years old, interested in starting light exercises. |
| **Zima Response** | Start slowly and gradually increase intensity. Choose low-impact activities like walking or swimming. Consult your doctor first. Begin with short sessions (10-15 minutes). Focus on balance and safety. Always warm up and cool down. Stay hydrated and listen to your body. If you feel pain, stop immediately. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response provides specific duration recommendations (10-15 minutes), emphasizes safety considerations (warm-up, pain cessation), and appropriately recommends physician consultation for new exercise regimens.

---

**Sample 3: Safety (Fall Prevention)**

| Component | Content |
|-----------|---------|
| **Instruction** | "How can I reduce my risk of falls at home?" |
| **Context** | Patient is elderly (70+), looking for practical advice. |
| **Zima Response** | Remove clutter from walkways. Install grab bars near toilet, shower, and bathtub. Use non-slip mats in bathrooms. Ensure good lighting throughout house. Wear sturdy shoes with rubber soles indoors. Talk to a professional about fall-proofing your living space. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response provides comprehensive, specific interventions with location guidance (bathroom fixtures) and practical footwear advice frequently overlooked by general models.

---

**Sample 4: Chronic Condition Management (Diabetes)**

| Component | Content |
|-----------|---------|
| **Instruction** | "How can I manage my blood sugar effectively?" |
| **Context** | Patient is 74 years old, newly diagnosed with Type 2 diabetes. |
| **Zima Response** | Monitor your diet closely - eat balanced meals and snacks. Exercise regularly but consult your doctor first. Take medications as prescribed. Regular check-ups are key. Stay hydrated. Limit sugary drinks. Consult your pharmacist for safe over-the-counter meds. Talk to your doctor before starting new treatments. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response appropriately emphasizes medication adherence, defers specific medical decisions to healthcare providers, and cautions about over-the-counter medication interactions.

---

**Sample 5: First Aid (Nosebleed)**

| Component | Content |
|-----------|---------|
| **Instruction** | "I have a nosebleed" |
| **Context** | Patient is elderly (70+), seeking first aid guidance. |
| **Zima Response** | Sit upright and lean forward slightly. Pinch nostrils shut for at least 10 minutes without letting go or tilting back. If bleeding doesn't stop, seek medical help immediately. Use ice packs on cheeks. Afterward, don't blow nose for several hours. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response provides correct first-aid technique (forward lean, 10-minute duration, no tilting back), clear escalation criteria, and appropriate aftercare instructions.

---

**Sample 6: Emotional Support (Grief)**

| Component | Content |
|-----------|---------|
| **Instruction** | "I am dealing with the loss of my spouse" |
| **Context** | Patient is 73 years old, recently lost their partner. |
| **Zima Response** | It's normal to feel overwhelmed and lonely. Seek support from family or friends, join a grief support group. Allow yourself time to grieve but also try new activities to keep busy. Remember it's okay not to be okay yet. Talk about your feelings openly. Consult a counselor if needed. |
| **Evaluation** | R: ✓ A: ✓ S: ✓ E: ✓ |

**Analysis:** The response demonstrates exceptional empathy ("It's okay not to be okay yet"), validates emotional experience, provides balanced coping suggestions, and recommends professional support when appropriate.

---

### 5.3 Comparative Analysis

Direct comparison between base model and fine-tuned model outputs reveals substantial improvements across all assessment dimensions:

**Table 7: Comparative Analysis: Base Model vs. Zima**

| Dimension | Base Model Behavior | Zima Behavior | Improvement |
|-----------|---------------------|---------------|-------------|
| Response Style | Generic, clinical terminology | Compassionate, senior-appropriate | Substantial |
| Actionability | Primarily "consult your doctor" | Home remedies with specific parameters | Major |
| Specificity | Vague recommendations | Concrete durations, dosages, frequencies | Major |
| Safety Integration | Inconsistent emergency recognition | Systematic escalation criteria | Moderate |
| Empathy Expression | Limited emotional acknowledgment | Warm, validating communication | Substantial |
| Average Response Length | 45 tokens | 87 tokens | 93% increase |
| Practical Steps per Response | 1-2 | 4-6 | 3× increase |

**Illustrative Comparison:**

*Query: "What can I do about constipation?"*
*Context: Patient is 73 years old, reports infrequent bowel movements.*

| Model | Response |
|-------|----------|
| **Base Qwen** | "Constipation can be caused by various factors. You should consult your doctor for proper evaluation and treatment recommendations." |
| **Zima** | "Drink 8-10 glasses of water daily. Eat more fiber: prunes, whole grains, vegetables. Walk for 20 minutes after meals. Try sitting on toilet same time each day. If no improvement in 3 days, contact your doctor." |

The fine-tuned model provides five specific interventions with quantified parameters, whereas the base model offers only a generic referral recommendation.

---

## 6. DISCUSSION

### 6.1 Findings

This research demonstrates that parameter-efficient fine-tuning through LoRA enables effective domain specialization for geriatric health assistance within practical computational constraints. Key findings include:

1. **Efficient Adaptation:** A 74 MB adapter achieves substantial domain specialization while preserving base model capabilities, validating LoRA's effectiveness for healthcare domain adaptation.

2. **Strong Perplexity Performance:** The achieved perplexity of 1.51 substantially exceeds typical baseline performance, indicating effective learning of target domain distributions.

3. **Qualitative Improvement:** Systematic evaluation demonstrates improvements in actionability, specificity, safety integration, and communicative empathy compared to the unmodified base model.

4. **Synthetic Data Viability:** GPU-accelerated synthetic data generation produced sufficient quality and quantity for effective model specialization, validating this approach for domains where authentic data may be scarce.

### 6.2 Limitations

Several limitations warrant acknowledgment:

1. **Synthetic Data Constraints:** Generated data, while high-quality, may not capture the full diversity of real-world patient interactions, rare conditions, or culturally-specific health practices.

2. **Monolingual Scope:** Current training data is exclusively English-language, limiting applicability to non-English-speaking elderly populations.

3. **Absence of Clinical Validation:** The model has not been evaluated by licensed healthcare professionals or tested in clinical settings with actual elderly patients.

4. **Static Knowledge Representation:** Model knowledge is frozen at training time, precluding access to updated medical guidelines, emerging treatments, or patient-specific medical histories.

5. **Base Model Capability Constraints:** The 1.5B parameter scale, while efficient for deployment, imposes limitations on complex multi-step reasoning capabilities.

6. **Evaluation Scope:** Qualitative evaluation, while systematic, relied on author assessment rather than independent clinical expert evaluation.

### 6.3 Ethical Considerations

The development and deployment of healthcare-oriented AI systems necessitates careful ethical reflection:

**Safety Guardrails:** The model is designed to recommend professional consultation for emergency symptoms (chest pain, stroke indicators, severe symptoms) and persistent conditions, avoiding inappropriate autonomous medical decision-making.

**Intended Use Context:** Zima is designed as an informational supplement to, not a replacement for, professional medical advice. Users should understand limitations and seek appropriate clinical care.

**Information Accuracy Disclaimer:** All model outputs should be considered informational only. Users should verify recommendations with qualified healthcare providers before implementation.

**Privacy Considerations:** The demonstration application processes queries locally without persistent storage, minimizing privacy risks. Production deployment would require comprehensive data protection measures.

---

## 7. CONCLUSION AND FUTURE WORK

### 7.1 Conclusion

This thesis presented Zima, a domain-specialized Large Language Model fine-tuned for geriatric health assistance. Through a comprehensive three-stage pipeline—synthetic data generation, LoRA fine-tuning, and interactive demonstration—we developed a model that provides compassionate, practical, and actionable health guidance tailored for elderly patients.

**Summary of Achievements:**

| Deliverable | Specification |
|-------------|---------------|
| Synthetic Dataset | 10,743 instruction-response pairs |
| Topic Coverage | 20 geriatric health categories |
| Model Perplexity | 1.51 |
| Adapter Size | 74 MB |
| Public Deployment | HuggingFace Hub |
| Demonstration | Gradio comparison interface |

The research demonstrates that parameter-efficient fine-tuning enables small-scale language models to achieve expert-level domain specialization with minimal computational overhead, presenting a viable pathway for developing specialized healthcare AI systems.

### 7.2 Future Work

Several directions merit future investigation:

1. **Clinical Validation:** Collaboration with geriatric healthcare professionals to conduct formal evaluation of response quality, safety, and clinical appropriateness.

2. **Multilingual Extension:** Expansion of training data to include Turkish, Spanish, and other languages to serve diverse elderly populations globally.

3. **Retrieval-Augmented Generation (RAG):** Integration with medical knowledge bases to enable access to current guidelines, drug interaction databases, and evidence-based recommendations.

4. **Voice Interface Development:** Implementation of speech-to-text and text-to-speech capabilities to enable hands-free interaction suitable for users with visual or motor limitations.

5. **Larger Model Application:** Extension of methodology to 7B and 14B parameter models to assess capability improvements from increased scale.

6. **Longitudinal Evaluation:** Deployment studies assessing real-world utility and user satisfaction among elderly populations over extended periods.

7. **Personalization Mechanisms:** Development of user profile integration enabling responses tailored to individual medical histories, preferences, and communication needs.

---

## REFERENCES

[1] United Nations, Department of Economic and Social Affairs, Population Division. (2022). *World Population Prospects 2022: Summary of Results*. UN DESA/POP/2022/TR/NO. 3.

[2] Fried, L. P., Tangen, C. M., Walston, J., Newman, A. B., Hirsch, C., Gottdiener, J., ... & McBurnie, M. A. (2001). Frailty in older adults: evidence for a phenotype. *The Journals of Gerontology Series A: Biological Sciences and Medical Sciences*, 56(3), M146-M157. https://doi.org/10.1093/gerona/56.3.M146

[3] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arber, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. *arXiv preprint arXiv:2108.07258*.

[4] OpenAI. (2023). GPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*. https://doi.org/10.48550/arXiv.2303.08774

[5] Anthropic. (2024). The Claude Model Card and Evaluations. https://www.anthropic.com/claude

[6] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). LLaMA: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*. https://doi.org/10.48550/arXiv.2302.13971

[7] Qwen Team, Alibaba Group. (2024). Qwen2.5 Technical Report. https://qwenlm.github.io/blog/qwen2.5/

[8] Nori, H., King, N., McKinney, S. M., Carignan, D., & Horvitz, E. (2023). Capabilities of GPT-4 on medical challenge problems. *arXiv preprint arXiv:2303.13375*. https://doi.org/10.48550/arXiv.2303.13375

[9] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., ... & Natarajan, V. (2023). Large language models encode clinical knowledge. *Nature*, 620(7972), 172-180. https://doi.org/10.1038/s41586-023-06291-2

[10] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., ... & Natarajan, V. (2023). Towards expert-level medical question answering with large language models. *arXiv preprint arXiv:2305.09617*.

[11] Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., & Ting, D. S. W. (2023). Large language models in medicine. *Nature Medicine*, 29(8), 1930-1940. https://doi.org/10.1038/s41591-023-02448-8

[12] Labrak, Y., Bazoge, A., Morin, E., Gourraud, P. A., Rouvier, M., & Dufour, R. (2024). BioMistral: A collection of open-source pretrained large language models for medical domains. *arXiv preprint arXiv:2402.10373*.

[13] Wu, C., Zhang, X., Zhang, Y., Wang, Y., & Xie, W. (2023). PMC-LLaMA: Towards building open-source language models for medicine. *arXiv preprint arXiv:2304.14454*.

[14] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*. https://doi.org/10.48550/arXiv.2106.09685

[15] Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient finetuning of quantized LLMs. *Advances in Neural Information Processing Systems*, 36. https://doi.org/10.48550/arXiv.2305.14314

[16] Han, D. (2023). Unsloth: Fast LLM fine-tuning. GitHub repository. https://github.com/unslothai/unsloth

[17] Long, S., Schuster, L., & Polu, S. (2024). Large language models for synthetic data generation. *arXiv preprint arXiv:2407.02142*.

[18] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2023). Self-instruct: Aligning language models with self-generated instructions. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*, 13484-13508. https://doi.org/10.18653/v1/2023.acl-long.754

[19] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., & Hashimoto, T. B. (2023). Stanford Alpaca: An instruction-following LLaMA model. GitHub repository. https://github.com/tatsu-lab/stanford_alpaca

[20] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 38-45. https://doi.org/10.18653/v1/2020.emnlp-demos.6

---

## APPENDICES

### Appendix A: Public Artifact Repositories

**Model Repository:**  
URL: https://huggingface.co/YsK-dev/zima-qwen-geriatric-1.5b  
License: Apache 2.0  
Contents: LoRA adapter weights, tokenizer files, configuration

**Dataset Repository:**  
URL: https://huggingface.co/datasets/YsK-dev/geriatric-health-advice  
License: Apache 2.0  
Contents: 10,743 instruction-response pairs in JSONL format

### Appendix B: Model Loading Code

```python
from unsloth import FastLanguageModel

# Load fine-tuned model from HuggingFace Hub
model, tokenizer = FastLanguageModel.from_pretrained(
    "YsK-dev/zima-qwen-geriatric-1.5b",
    max_seq_length=512,
    load_in_4bit=True,
)

# Prepare for inference
FastLanguageModel.for_inference(model)

# Generate response
def generate_response(instruction: str, context: str) -> str:
    prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{context}

### Response:
"""
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### Appendix C: Replication Instructions

```bash
# Clone repository
git clone https://github.com/YsK-dev/zima.git
cd zima

# Stage 1: Data Generation (requires Lightning.ai with L40 GPU)
cd data_creation
./setup_lightning.sh
python data_creation_lightning.py

# Stage 2: Model Training
cd ../training
./setup_training.sh
python prepare_data.py
python train_unsloth.py
python evaluate_model.py

# Stage 3: Demonstration
cd ../demo
pip install -r requirements.txt
python app.py
```

### Appendix D: Adapter Configuration

```json
{
  "base_model_name_or_path": "unsloth/qwen2.5-1.5b-instruct-unsloth-bnb-4bit",
  "peft_type": "LORA",
  "r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "target_modules": [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
  ],
  "bias": "none",
  "task_type": "CAUSAL_LM"
}
```

---

**HOW TO CITE THIS WORK:**

Sertkaya, Y., & Sezer, U. (2025). *Zima: A domain-specialized large language model for geriatric health assistance through low-rank adaptation* [Undergraduate thesis, Aydın Adnan Menderes University]. HuggingFace Hub. https://huggingface.co/YsK-dev/zima-qwen-geriatric-1.5b

---

**BibTeX:**

```bibtex
@thesis{sertkaya2025zima,
  author       = {Sertkaya, Yusuf and Sezer, Umut},
  title        = {Zima: A Domain-Specialized Large Language Model for 
                  Geriatric Health Assistance Through Low-Rank Adaptation},
  school       = {Aydın Adnan Menderes University},
  year         = {2025},
  type         = {Bachelor's Thesis},
  address      = {Aydın, Turkey},
  month        = {January},
  note         = {Department of Computer Engineering, Faculty of Engineering},
  url          = {https://huggingface.co/YsK-dev/zima-qwen-geriatric-1.5b}
}
```

---

© 2025 Yusuf Sertkaya & Umut Sezer  
Aydın Adnan Menderes University  
Faculty of Engineering, Department of Computer Engineering
