# ğŸ¥ Zima - Geriatric Health Assistant

A fine-tuned AI model to provide compassionate, actionable health advice for elderly individuals (70+).

## ğŸ¯ Project Overview

**Model**: Qwen 2.5 1.5B fine-tuned with LoRA  
**Dataset**: 10,743 synthetic samples  
**Perplexity**: 1.51 (excellent)  

## ğŸ”— HuggingFace

- **Model**: [YsK-dev/zima-qwen-geriatric-1.5b](https://huggingface.co/YsK-dev/zima-qwen-geriatric-1.5b)
- **Dataset**: [YsK-dev/geriatric-health-advice](https://huggingface.co/datasets/YsK-dev/geriatric-health-advice)

## ğŸ“ Project Structure

```
zima/
â”œâ”€â”€ data_creation/           # Data generation scripts
â”‚   â”œâ”€â”€ data_creation_lightning.py  # Main generation script
â”‚   â”œâ”€â”€ setup_lightning.sh          # Lightning.ai setup
â”‚   â””â”€â”€ LIGHTNING_AI_GUIDE.md       # Usage guide
â”‚
â”œâ”€â”€ training/                # Model training
â”‚   â”œâ”€â”€ prepare_data.py      # Data preprocessing
â”‚   â”œâ”€â”€ train_unsloth.py     # Unsloth fine-tuning
â”‚   â”œâ”€â”€ evaluate_model.py    # Model evaluation
â”‚   â””â”€â”€ setup_training.sh    # Environment setup
â”‚
â”œâ”€â”€ generated_data/          # Training data
â”‚   â””â”€â”€ synthetic_geriatric_data (2).jsonl
â”‚
â”œâ”€â”€ seed_data/               # Initial seed prompts
â”‚
â”œâ”€â”€ trainde_model/Model files (LoRA adapter):
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ tokenizer files
â”‚
â”œâ”€â”€ MODEL_CARD.md            # HuggingFace model docs
â”œâ”€â”€ DATASET_CARD.md          # HuggingFace dataset docs
```

## ğŸš€ Quick Start

### Load from HuggingFace

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    "YsK-dev/zima-qwen-geriatric-1.5b",
    max_seq_length=512,
    load_in_4bit=True,
)
```

### Replicate Training

```bash
# 1. Generate data (Lightning.ai)
cd data_creation
./setup_lightning.sh
python data_creation_lightning.py

# 2. Train model (Lightning.ai)
cd training
./setup_training.sh
python prepare_data.py
python train_unsloth.py
python evaluate_model.py
```

## ğŸ“Š Results

| Metric | Value |
|--------|-------|
| Training Loss | 0.32 |
| Validation Loss | 0.40 |
| Perplexity | 1.51 |
| Samples | 10,743 |

## ğŸ“„ License

Apache 2.0